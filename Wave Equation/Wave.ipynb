{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import traceback\n",
    "from collections import OrderedDict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 20230808\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Perform garbage collection and empty the GPU cache in PyTorch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network class.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input features.\n",
    "        hidden_size (int): The number of hidden units in each hidden layer.\n",
    "        output_size (int): The number of output features.\n",
    "        depth (int): The number of hidden layers.\n",
    "        ac (torch.nn.Module): The activation function to use for each hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        depth,\n",
    "        ac=torch.nn.Tanh,\n",
    "    ):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        layers = [('input', torch.nn.Linear(input_size, hidden_size))]\n",
    "        layers.append(('input_activation', ac()))\n",
    "        for i in range(depth):\n",
    "            layers.append(\n",
    "                ('hidden_%d' % i, torch.nn.Linear(hidden_size, hidden_size))\n",
    "            )\n",
    "            layers.append(('activation_%d' % i, ac()))\n",
    "        layers.append(('output', torch.nn.Linear(hidden_size, output_size)))\n",
    "\n",
    "        layerDict = OrderedDict(layers)\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "A one-dimensional wave equation is chosen for our experiments, which, in mathematical form, is defined as follows:\n",
    "\n",
    "$$ u_{tt} - u_{xx} = 0$$\n",
    "\n",
    "for this wave equation, its initial conditions and the homogeneous Dirichlet boundary conditions are given, as follows:\n",
    "\n",
    "$$ u(0, x) = \\frac{1}{2} \\sin (\\pi x)$$\n",
    "$$ u_t(0, x) = \\pi \\sin (3 \\pi x)$$\n",
    "$$ u(t, 0) =  u(t, 1) = 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    \"\"\"\n",
    "    Initializes the weights of a layer with Xavier normal initialization.\n",
    "    Args:\n",
    "        layer (torch.nn.Module): The layer to initialize.\n",
    "    \"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "\n",
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.model = NN(\n",
    "            input_size=2,\n",
    "            hidden_size=100,\n",
    "            output_size=1,\n",
    "            depth=6,\n",
    "            ac=torch.nn.Tanh\n",
    "        ).to(device)\n",
    "\n",
    "        # use the Glorot normal initializer for initialization\n",
    "        self.model.apply(init_weights)\n",
    "\n",
    "        # The initial conditions, boundary conditions\n",
    "        # with Nu approximating 300\n",
    "        self.h = 0.1\n",
    "        self.k = 0.1\n",
    "        x = torch.arange(0, 1 + self.h, self.h)\n",
    "        t = torch.arange(0, 1 + self.k, self.k)\n",
    "        # x[0] = 0, x[-1] = 1, t[0] = 0\n",
    "        bc1 = torch.stack(torch.meshgrid(x[0], t)).reshape(2, -1).T\n",
    "        bc2 = torch.stack(torch.meshgrid(x[-1], t)).reshape(2, -1).T\n",
    "        ic = torch.stack(torch.meshgrid(x, t[0])).reshape(2, -1).T\n",
    "        self.X_train = torch.cat([bc1, bc2, ic])\n",
    "        y_bc1 = torch.zeros(len(bc1))\n",
    "        y_bc2 = torch.zeros(len(bc2))\n",
    "        y_ic = 1/2*torch.sin(math.pi * ic[:, 0])\n",
    "        self.y_train = torch.cat([y_bc1, y_bc2, y_ic])\n",
    "        self.y_train = self.y_train.unsqueeze(1)\n",
    "\n",
    "        # Data in the space-time domain\n",
    "        # with Nf approximating 40000\n",
    "        self.h = 0.005\n",
    "        self.k = 0.005\n",
    "        x = torch.arange(self.h, 1 + self.h, self.h)\n",
    "        t = torch.arange(self.k, 1 + self.k, self.k)\n",
    "        self.X = torch.stack(torch.meshgrid(x, t)).reshape(2, -1).T\n",
    "\n",
    "        # Device Consistency\n",
    "        self.X = self.X.to(device)\n",
    "        self.X_train = self.X_train.to(device)\n",
    "        self.y_train = self.y_train.to(device)\n",
    "        self.X.requires_grad = True\n",
    "\n",
    "        # Logger\n",
    "        self.iter = 1\n",
    "\n",
    "        # Loss Function\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        # Two Optimizer\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.model.parameters(),\n",
    "            lr=0.001,\n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-7, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "        self.adam = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "    def loss_func(self):\n",
    "        self.adam.zero_grad()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # loss using observations of initial and boundary conditions\n",
    "        y_pred = self.model(self.X_train)\n",
    "        loss_data = self.criterion(y_pred, self.y_train)\n",
    "\n",
    "        # loss based on partial differential equations\n",
    "        u = self.model(self.X)\n",
    "\n",
    "        du_dX = torch.autograd.grad(\n",
    "            inputs=self.X,\n",
    "            outputs=u,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        du_dt = du_dX[:, 1]\n",
    "        du_dx = du_dX[:, 0]\n",
    "\n",
    "        du_dxx = torch.autograd.grad(\n",
    "            inputs=self.X,\n",
    "            outputs=du_dX,\n",
    "            grad_outputs=torch.ones_like(du_dX),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0][:, 0]\n",
    "\n",
    "        du_dtt = torch.autograd.grad(\n",
    "            inputs=self.X,\n",
    "            outputs=du_dt,\n",
    "            grad_outputs=torch.ones_like(du_dt),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0][:, 1]\n",
    "\n",
    "        loss_pde = self.criterion(du_dtt - du_dxx, du_dxx*0)\n",
    "\n",
    "        loss = loss_pde + loss_data\n",
    "        loss.backward()\n",
    "        if self.iter % 100 == 0:\n",
    "            print(self.iter, loss.item())\n",
    "        self.iter = self.iter + 1\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        # L-BFGS 30,000 epochs\n",
    "        for _ in tqdm(range(20)):\n",
    "            self.optimizer.step(self.loss_func)\n",
    "            # continued the optimization using Adam\n",
    "            for _ in range(1500):\n",
    "                self.adam.step(self.loss_func)\n",
    "\n",
    "    def eval_(self):\n",
    "        self.model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15119\\miniconda3\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004049777984619141,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4f705cee954396bef03ab8946f7716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.02055489830672741\n",
      "200 0.019409509375691414\n",
      "300 0.018849054351449013\n",
      "400 0.011236368678510189\n",
      "500 0.005986572243273258\n",
      "600 0.004425127990543842\n",
      "700 0.002523630391806364\n",
      "800 0.001528450520709157\n",
      "900 0.0013338442659005523\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004016876220703125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1500,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2860e6b818b2459ba96825305f881f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.2565414309501648\n",
      "1100 0.01664217747747898\n",
      "1200 0.00883091613650322\n",
      "1300 0.0019051822600886226\n",
      "1400 0.001368343597277999\n",
      "1500 0.0012860839487984776\n",
      "1600 0.001252887537702918\n",
      "1700 0.001237573567777872\n",
      "1800 0.0012410100316628814\n",
      "1900 0.007598113268613815\n",
      "2000 0.0012101908214390278\n",
      "2100 0.001211515162140131\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m Net()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 128\u001b[0m, in \u001b[0;36mNet.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# continued the optimization using Adam\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1500\u001b[39m)):\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 121\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    124\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[3], line 96\u001b[0m, in \u001b[0;36mNet.loss_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m du_dt \u001b[38;5;241m=\u001b[39m du_dX[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     94\u001b[0m du_dx \u001b[38;5;241m=\u001b[39m du_dX[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 96\u001b[0m du_dxx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdu_dX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdu_dX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    104\u001b[0m du_dtt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[0;32m    105\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX,\n\u001b[0;32m    106\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mdu_dt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    110\u001b[0m )[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    112\u001b[0m loss_pde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(du_dtt \u001b[38;5;241m-\u001b[39m du_dxx, du_dxx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.model.state_dict(), 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import plotly.express as px\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# plt.style.use(['ipynb', 'use_mathtext', 'colors5-light', 'science'])\n",
    "# sns.set_style('whitegrid')\n",
    "# sns.set_palette('RdBu')\n",
    "# sns.set(\n",
    "#     rc={'text.usetex': True},\n",
    "#     font='serif',\n",
    "#     font_scale=1.2\n",
    "# )\n",
    "\n",
    "# matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# matplotlib.rcParams['font.serif'] = ['SimHei']\n",
    "# sns.set_style('darkgrid', {'font.sans-serif': ['simhei', 'Arial']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "k = 0.001\n",
    "x = torch.arange(0, 1, h)\n",
    "t = torch.arange(0, 1, k)\n",
    "X = torch.stack(torch.meshgrid(x, t)).reshape(2, -1).T\n",
    "X = X.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(X):\n",
    "    x = X[:, 0]\n",
    "    t = X[:, 1]\n",
    "    y = 1/2*torch.sin(torch.pi*x)*torch.cos(torch.pi*t)+1 / \\\n",
    "        3*torch.sin(3*torch.pi*x)*torch.sin(3*torch.pi*t)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NN(input_size=2,\n",
    "           hidden_size=100,\n",
    "           output_size=1,\n",
    "           depth=6,\n",
    "           ac=torch.nn.Tanh).to(device)\n",
    "model.load_state_dict(torch.load('model.ckpt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # y_pred = ground_truth(X).reshape(len(x), len(t)).detach().cpu()\n",
    "    y_pred = model(X).reshape(len(x), len(t)).detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3), dpi=150)\n",
    "sns.heatmap(y_pred, cmap='jet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0, 1, h).unsqueeze(dim=1).to(device)\n",
    "t = (torch.ones(x.shape)*0.5).to(device)\n",
    "\n",
    "plt.scatter(x.detach().cpu(), model(torch.cat([x, t], dim=1)).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
